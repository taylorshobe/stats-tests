{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "import pandasql as psql\n",
    "from pandasql import sqldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-67b859071fbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;31m# Perform the T-Tests and get the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m \u001b[0mt_test_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperform_t_tests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_test_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the function to perform T-Tests and calculate Cohen's d\n",
    "def perform_t_tests(df, metrics, alpha=0.05):\n",
    "    # Get unique values for 'area' and 'tenured'\n",
    "    areas = df['area'].unique()\n",
    "    tenured_statuses = df['tenured'].unique()\n",
    "    \n",
    "    # Function to interpret Cohen's d\n",
    "    def interpret_cohen_d(d):\n",
    "        if abs(d) < 0.2:\n",
    "            return \"Small\"\n",
    "        elif abs(d) < 0.5:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Large\"\n",
    "\n",
    "    # Initialize a list to store results\n",
    "    results = []\n",
    "\n",
    "    for tenured in tenured_statuses:\n",
    "        for area in areas:\n",
    "            for metric in metrics:\n",
    "                # Subset data based on 'area' and 'tenured'\n",
    "                subset = df[(df['tenured'] == tenured) & (df['area'] == area)]\n",
    "                # Separate data into 'HOME' and 'OFFICE' groups\n",
    "                wfh_data = subset[subset['day_type'] == 'HOME'][metric]\n",
    "                office_data = subset[subset['day_type'] == 'OFFICE'][metric]\n",
    "                \n",
    "                if len(wfh_data) > 0 and len(office_data) > 0:\n",
    "                    # Perform T-Test\n",
    "                    t_stat, p_value = ttest_ind(wfh_data, office_data, equal_var=False, nan_policy='omit')\n",
    "                    \n",
    "                    # Calculate means and sample sizes\n",
    "                    wfh_mean = wfh_data.mean()\n",
    "                    office_mean = office_data.mean()\n",
    "                    wfh_std = wfh_data.std()\n",
    "                    office_std = office_data.std()\n",
    "                    n_wfh = len(wfh_data)\n",
    "                    n_office = len(office_data)\n",
    "                    \n",
    "                    # Calculate pooled standard deviation\n",
    "                    pooled_std = np.sqrt(((n_wfh - 1) * wfh_std**2 + (n_office - 1) * office_std**2) / (n_wfh + n_office - 2))\n",
    "                    \n",
    "                    # Calculate Cohen's d\n",
    "                    cohen_d = (wfh_mean - office_mean) / pooled_std\n",
    "                    cohen_d_interpretation = interpret_cohen_d(cohen_d)\n",
    "                    \n",
    "                    # Append results to the list\n",
    "                    results.append({\n",
    "                        'Tenured': tenured,\n",
    "                        'Area': area,\n",
    "                        'Metric': metric,\n",
    "                        'WFH Mean': wfh_mean,\n",
    "                        'Office Mean': office_mean,\n",
    "                        'T-Statistic': t_stat,\n",
    "                        'P-Value': round(p_value, 5),\n",
    "                        'Cohen D': round(cohen_d, 5),\n",
    "                        'Cohen D Interpretation': cohen_d_interpretation\n",
    "                    })\n",
    "        \n",
    "        # Add combined area for the current tenured status\n",
    "        for metric in metrics:\n",
    "            combined_subset = df[df['tenured'] == tenured]\n",
    "            wfh_data_combined = combined_subset[combined_subset['day_type'] == 'HOME'][metric]\n",
    "            office_data_combined = combined_subset[combined_subset['day_type'] == 'OFFICE'][metric]\n",
    "            \n",
    "            if len(wfh_data_combined) > 0 and len(office_data_combined) > 0:\n",
    "                # Perform T-Test\n",
    "                t_stat_combined, p_value_combined = ttest_ind(wfh_data_combined, office_data_combined, equal_var=False, nan_policy='omit')\n",
    "                \n",
    "                # Calculate means and sample sizes\n",
    "                wfh_mean_combined = wfh_data_combined.mean()\n",
    "                office_mean_combined = office_data_combined.mean()\n",
    "                wfh_std_combined = wfh_data_combined.std()\n",
    "                office_std_combined = office_data_combined.std()\n",
    "                n_wfh_combined = len(wfh_data_combined)\n",
    "                n_office_combined = len(office_data_combined)\n",
    "                \n",
    "                # Calculate pooled standard deviation\n",
    "                pooled_std_combined = np.sqrt(((n_wfh_combined - 1) * wfh_std_combined**2 + (n_office_combined - 1) * office_std_combined**2) / (n_wfh_combined + n_office_combined - 2))\n",
    "                \n",
    "                # Calculate Cohen's d\n",
    "                cohen_d_combined = (wfh_mean_combined - office_mean_combined) / pooled_std_combined\n",
    "                cohen_d_combined_interpretation = interpret_cohen_d(cohen_d_combined)\n",
    "                \n",
    "                # Append combined area results to the list\n",
    "                results.append({\n",
    "                    'Tenured': tenured,\n",
    "                    'Area': 'All Areas Combined',\n",
    "                    'Metric': metric,\n",
    "                    'WFH Mean': wfh_mean_combined,\n",
    "                    'Office Mean': office_mean_combined,\n",
    "                    'T-Statistic': t_stat_combined,\n",
    "                    'P-Value': round(p_value_combined, 5),\n",
    "                    'Cohen D': round(cohen_d_combined, 5),\n",
    "                    'Cohen D Interpretation': cohen_d_combined_interpretation\n",
    "                })\n",
    "    \n",
    "    # Convert results list to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Assuming 'result_df' is your DataFrame loaded with your data\n",
    "metrics = ['avg_interaction_count', 'aht', 'avg_productivity']\n",
    "\n",
    "# Set the alpha level (e.g., 0.05 for a 95% confidence level)\n",
    "alpha = 0.05\n",
    "\n",
    "# Perform the T-Tests and get the results\n",
    "t_test_results = perform_t_tests(result_df, metrics)\n",
    "\n",
    "print(t_test_results.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from math import asin, sqrt\n",
    "\n",
    "# Transcribe the data into a list of dictionaries\n",
    "data = [\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Chat\", \"Sample\": 4, \"Metric\": \"Adherence\", \"WFH Proportion\": 0.755},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Chat\", \"Sample\": 4, \"Metric\": \"Consults\", \"WFH Proportion\": 0.153},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Chat\", \"Sample\": 4, \"Metric\": \"OSAT\", \"WFH Proportion\": 0.552},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Chat\", \"Sample\": 4, \"Metric\": \"Transfers\", \"WFH Proportion\": 0.129},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Client Banking Services\", \"Sample\": 46, \"Metric\": \"Adherence\", \"WFH Proportion\": 0.912},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Client Banking Services\", \"Sample\": 46, \"Metric\": \"Consults\", \"WFH Proportion\": 0.206},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Client Banking Services\", \"Sample\": 46, \"Metric\": \"OSAT\", \"WFH Proportion\": 0.472},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Client Banking Services\", \"Sample\": 46, \"Metric\": \"Transfers\", \"WFH Proportion\": 0.148},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Core Service\", \"Sample\": 594, \"Metric\": \"Adherence\", \"WFH Proportion\": 0.836},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Core Service\", \"Sample\": 594, \"Metric\": \"Consults\", \"WFH Proportion\": 0.178},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Core Service\", \"Sample\": 594, \"Metric\": \"OSAT\", \"WFH Proportion\": 0.468},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Core Service\", \"Sample\": 594, \"Metric\": \"Transfers\", \"WFH Proportion\": 0.188},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Tier 2 Support\", \"Sample\": 5, \"Metric\": \"Adherence\", \"WFH Proportion\": 0.888},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Tier 2 Support\", \"Sample\": 5, \"Metric\": \"Consults\", \"WFH Proportion\": 0.104},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Tier 2 Support\", \"Sample\": 5, \"Metric\": \"OSAT\", \"WFH Proportion\": 1.000},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Tier 2 Support\", \"Sample\": 5, \"Metric\": \"Transfers\", \"WFH Proportion\": 0.241},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Trader Service\", \"Sample\": 6, \"Metric\": \"Adherence\", \"WFH Proportion\": 0.822},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Trader Service\", \"Sample\": 6, \"Metric\": \"Consults\", \"WFH Proportion\": 0.266},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Trader Service\", \"Sample\": 6, \"Metric\": \"OSAT\", \"WFH Proportion\": 0.571},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Trader Service\", \"Sample\": 6, \"Metric\": \"Transfers\", \"WFH Proportion\": 0.273},\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Adding some hypothetical Office Proportion data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "df[\"Office Proportion\"] = np.random.rand(len(df))\n",
    "\n",
    "# Set the alpha level (e.g., 0.05 for a 95% confidence level)\n",
    "alpha = 0.05\n",
    "\n",
    "# Function to interpret Cohen's h\n",
    "def interpret_cohen_h(h):\n",
    "    if abs(h) < 0.2:\n",
    "        return \"Small\"\n",
    "    elif abs(h) < 0.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\"\n",
    "\n",
    "# Perform Z-tests for each row\n",
    "results = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    count = np.array([row[\"WFH Proportion\"] * row[\"Sample\"], row[\"Office Proportion\"] * row[\"Sample\"]])\n",
    "    nobs = np.array([row[\"Sample\"], row[\"Sample\"]])\n",
    "    \n",
    "    z_score, p_value = proportions_ztest(count, nobs)\n",
    "    \n",
    "    # Calculate Cohen's h\n",
    "    cohen_h = 2 * (asin(sqrt(row[\"WFH Proportion\"])) - asin(sqrt(row[\"Office Proportion\"])))\n",
    "    \n",
    "    results.append({\n",
    "        \"Tenured\": row[\"Tenured\"],\n",
    "        \"Area Name\": row[\"Area Name\"],\n",
    "        \"Sample\": row[\"Sample\"],\n",
    "        \"Metric\": row[\"Metric\"],\n",
    "        \"WFH Proportion\": row[\"WFH Proportion\"],\n",
    "        \"Office Proportion\": row[\"Office Proportion\"],\n",
    "        \"Z-Statistic\": z_score,\n",
    "        \"P-Value\": p_value,\n",
    "        \"Cohen H\": cohen_h,\n",
    "        \"Cohen H Interpretation\": interpret_cohen_h(cohen_h),\n",
    "        \"Significant\": p_value < alpha  # Check if the p-value is less than the alpha level\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Add combined area for each tenured status\n",
    "combined_results = []\n",
    "\n",
    "for tenured in df['Tenured'].unique():\n",
    "    for metric in df['Metric'].unique():\n",
    "        combined_subset = df[(df['Tenured'] == tenured) & (df['Metric'] == metric)]\n",
    "        combined_sample = combined_subset['Sample'].sum()\n",
    "        combined_wfh_proportion = (combined_subset['WFH Proportion'] * combined_subset['Sample']).sum() / combined_sample\n",
    "        combined_office_proportion = (combined_subset['Office Proportion'] * combined_subset['Sample']).sum() / combined_sample\n",
    "        \n",
    "        count_combined = np.array([combined_wfh_proportion * combined_sample, combined_office_proportion * combined_sample])\n",
    "        nobs_combined = np.array([combined_sample, combined_sample])\n",
    "        \n",
    "        z_score_combined, p_value_combined = proportions_ztest(count_combined, nobs_combined)\n",
    "        \n",
    "        # Calculate Cohen's h for combined data\n",
    "        cohen_h_combined = 2 * (asin(sqrt(combined_wfh_proportion)) - asin(sqrt(combined_office_proportion)))\n",
    "        \n",
    "        combined_results.append({\n",
    "            \"Tenured\": tenured,\n",
    "            \"Area Name\": \"COMBINED\",\n",
    "            \"Sample\": combined_sample,\n",
    "            \"Metric\": metric,\n",
    "            \"WFH Proportion\": combined_wfh_proportion,\n",
    "            \"Office Proportion\": combined_office_proportion,\n",
    "            \"Z-Statistic\": z_score_combined,\n",
    "            \"P-Value\": p_value_combined,\n",
    "            \"Cohen H\": cohen_h_combined,\n",
    "            \"Cohen H Interpretation\": interpret_cohen_h(cohen_h_combined),\n",
    "            \"Significant\": p_value_combined < alpha  # Check if the p-value is less than the alpha level\n",
    "        })\n",
    "\n",
    "# Convert combined results to a DataFrame and append to the original results\n",
    "df_combined_results = pd.DataFrame(combined_results)\n",
    "df_final_results = pd.concat([df_results, df_combined_results], ignore_index=True)\n",
    "\n",
    "# Reorder the columns\n",
    "df_final_results = df_final_results[[\"Tenured\", \"Area Name\", \"Sample\", \"Metric\", \"WFH Proportion\", \"Office Proportion\", \"Z-Statistic\", \"P-Value\", \"Cohen H Interpretation\", \"Significant\"]]\n",
    "\n",
    "print(df_final_results.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-db1c0faefc95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m }\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m \u001b[0marea_tenure_z_test_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marea_tenure_z_tests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[0mcombined_z_test_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_z_tests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from math import asin, sqrt\n",
    "\n",
    "# Define WFH and Office days\n",
    "wfh_days = ['Tuesday', 'Friday']\n",
    "office_days = ['Monday', 'Wednesday', 'Thursday']\n",
    "\n",
    "# Function to interpret Cohen's h\n",
    "def interpret_cohen_h(h):\n",
    "    if abs(h) < 0.2:\n",
    "        return \"Small\"\n",
    "    elif abs(h) < 0.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\"\n",
    "\n",
    "# Function to get the overall difference in proportions by area and tenure\n",
    "def area_tenure_z_tests(df, metrics_dict):\n",
    "    area_names = df['area_name'].unique()\n",
    "    tenured_statuses = df['tenured'].unique()\n",
    "    results = []\n",
    "\n",
    "    for tenured in tenured_statuses:\n",
    "        for area in area_names:\n",
    "            for metric, (num_col, den_col) in metrics_dict.items():\n",
    "                subset = df[(df['tenured'] == tenured) & (df['area_name'] == area)]\n",
    "                wfh_subset = subset[subset['day'].isin(wfh_days)]\n",
    "                office_subset = subset[subset['day'].isin(office_days)]\n",
    "                wfh_numerator = wfh_subset[num_col].sum()\n",
    "                wfh_denominator = wfh_subset[den_col].sum()\n",
    "                office_numerator = office_subset[num_col].sum()\n",
    "                office_denominator = office_subset[den_col].sum()\n",
    "\n",
    "                if wfh_denominator == 0 or office_denominator == 0:\n",
    "                    continue\n",
    "\n",
    "                count = np.array([wfh_numerator, office_numerator])\n",
    "                observations = np.array([wfh_denominator, office_denominator])\n",
    "                z_stat, p_value = proportions_ztest(count, observations)\n",
    "                wfh_proportion = wfh_numerator / wfh_denominator if wfh_denominator else None\n",
    "                office_proportion = office_numerator / office_denominator if office_denominator else None\n",
    "\n",
    "                # Calculate Cohen's h\n",
    "                cohen_h = 2 * (asin(sqrt(wfh_proportion)) - asin(sqrt(office_proportion)))\n",
    "                cohen_h_interpretation = interpret_cohen_h(cohen_h)\n",
    "\n",
    "                results.append({\n",
    "                    'Tenured': tenured,\n",
    "                    'Area Name': area,\n",
    "                    'Metric': metric,\n",
    "                    'WFH Proportion': wfh_proportion,\n",
    "                    'Office Proportion': office_proportion,\n",
    "                    'Z-Statistic': z_stat,\n",
    "                    'P-Value': p_value.round(5),\n",
    "                    'Cohen H': round(cohen_h, 5),\n",
    "                    'Cohen H Interpretation': cohen_h_interpretation\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Function to get the overall difference in proportions by tenure (area agnostic)\n",
    "def combined_z_tests(df, metrics_dict):\n",
    "    tenured_statuses = df['tenured'].unique()\n",
    "    results = []\n",
    "\n",
    "    for tenured in tenured_statuses:\n",
    "        for metric, (num_col, den_col) in metrics_dict.items():\n",
    "            subset = df[df['tenured'] == tenured]\n",
    "            wfh_subset = subset[subset['day'].isin(wfh_days)]\n",
    "            office_subset = subset[subset['day'].isin(office_days)]\n",
    "            wfh_numerator = wfh_subset[num_col].sum()\n",
    "            wfh_denominator = wfh_subset[den_col].sum()\n",
    "            office_numerator = office_subset[num_col].sum()\n",
    "            office_denominator = office_subset[den_col].sum()\n",
    "\n",
    "            if wfh_denominator == 0 or office_denominator == 0:\n",
    "                continue\n",
    "\n",
    "            count = np.array([wfh_numerator, office_numerator])\n",
    "            observations = np.array([wfh_denominator, office_denominator])\n",
    "            z_stat, p_value = proportions_ztest(count, observations)\n",
    "            wfh_proportion = wfh_numerator / wfh_denominator if wfh_denominator else None\n",
    "            office_proportion = office_numerator / office_denominator if office_denominator else None\n",
    "\n",
    "            # Calculate Cohen's h\n",
    "            cohen_h = 2 * (asin(sqrt(wfh_proportion)) - asin(sqrt(office_proportion)))\n",
    "            cohen_h_interpretation = interpret_cohen_h(cohen_h)\n",
    "\n",
    "            results.append({\n",
    "                'Tenured': tenured,\n",
    "                'Area Name': \"COMBINED\",\n",
    "                'Metric': metric,\n",
    "                'WFH Proportion': wfh_proportion,\n",
    "                'Office Proportion': office_proportion,\n",
    "                'Z-Statistic': z_stat,\n",
    "                'P-Value': p_value.round(5),\n",
    "                'Cohen H': round(cohen_h, 5),\n",
    "                'Cohen H Interpretation': cohen_h_interpretation\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test\n",
    "\n",
    "metrics_dict = {\n",
    "    'Adherence': ('adh_num', 'adh_den'),\n",
    "    'OSAT': ('top_box', 'osat_count'),\n",
    "    'Transfers': ('transfers', 'interaction_count'),\n",
    "    'Consults': ('consults', 'interaction_count')\n",
    "}\n",
    "\n",
    "area_tenure_z_test_df = area_tenure_z_tests(result_df, metrics_dict)\n",
    "combined_z_test_df = combined_z_tests(result_df, metrics_dict)\n",
    "\n",
    "z_test = pd.concat([area_tenure_z_test_df, combined_z_test_df])\n",
    "\n",
    "print(z_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   day_type        date person_id       area_name tenured  talk_time  \\\n",
      "0    OFFICE  2024-01-03   A003083  High Net Worth     Yes       2353   \n",
      "1    OFFICE  2024-01-03   A003091            Chat     Yes      34945   \n",
      "2    OFFICE  2024-01-03   A003434  High Net Worth     Yes       6693   \n",
      "3    OFFICE  2024-01-03   A003687  Trader Service     Yes      12467   \n",
      "4    OFFICE  2024-01-03   A003771  High Net Worth     Yes       6444   \n",
      "5      HOME  2024-01-03   A004173    Core Service     Yes      18897   \n",
      "6      HOME  2024-01-03   A004173    Core Service     Yes      18897   \n",
      "7      HOME  2024-01-03   A004432  Trader Service     Yes      11115   \n",
      "8      HOME  2024-01-03   A005648  Trader Service     Yes       7036   \n",
      "9      HOME  2024-01-03   A007225            Chat     Yes      13667   \n",
      "10   OFFICE  2024-01-03   A008689  High Net Worth     Yes       3786   \n",
      "11     HOME  2024-01-03   A009607    Core Service     Yes      12718   \n",
      "12   OFFICE  2024-01-03   A009687    Core Service     Yes      13221   \n",
      "13     HOME  2024-01-03   A015864  High Net Worth     Yes       2695   \n",
      "14   OFFICE  2024-01-03   A017412    Core Service     Yes       2200   \n",
      "\n",
      "    wrap_time  interaction_count  total_head_ct  tenure_head_ct  \\\n",
      "0         749                  9           2468            1840   \n",
      "1           0                 51           2468            1840   \n",
      "2        1402                 12           2468            1840   \n",
      "3        4207                 26           2468            1840   \n",
      "4          84                  7           2468            1840   \n",
      "5        3399                 33           2468            1840   \n",
      "6        3399                 33           2468            1840   \n",
      "7        1736                 14           2468            1840   \n",
      "8        4361                 22           2468            1840   \n",
      "9           0                 18           2468            1840   \n",
      "10         27                 10           2468            1840   \n",
      "11       2010                 15           2468            1840   \n",
      "12        663                 18           2468            1840   \n",
      "13         27                  9           2468            1840   \n",
      "14        749                  4           2468            1840   \n",
      "\n",
      "    area_tenure_head_ct  \n",
      "0                   241  \n",
      "1                   209  \n",
      "2                   241  \n",
      "3                   326  \n",
      "4                   241  \n",
      "5                   764  \n",
      "6                   764  \n",
      "7                   326  \n",
      "8                   326  \n",
      "9                   209  \n",
      "10                  241  \n",
      "11                  764  \n",
      "12                  764  \n",
      "13                  241  \n",
      "14                  764  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for the DataFrame\n",
    "data = {\n",
    "    'day_type': [\n",
    "        'OFFICE', 'OFFICE', 'OFFICE', 'OFFICE', 'OFFICE',\n",
    "        'HOME', 'HOME', 'HOME', 'HOME', 'HOME',\n",
    "        'OFFICE', 'HOME', 'OFFICE', 'HOME', 'OFFICE'\n",
    "    ],\n",
    "    'date': [\n",
    "        '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03', \n",
    "        '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03', \n",
    "        '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03'\n",
    "    ],\n",
    "    'person_id': [\n",
    "        'A003083', 'A003091', 'A003434', 'A003687', 'A003771', \n",
    "        'A004173', 'A004173', 'A004432', 'A005648', 'A007225', \n",
    "        'A008689', 'A009607', 'A009687', 'A015864', 'A017412'\n",
    "    ],\n",
    "    'area_name': [\n",
    "        'High Net Worth', 'Chat', 'High Net Worth', 'Trader Service', 'High Net Worth', \n",
    "        'Core Service', 'Core Service', 'Trader Service', 'Trader Service', 'Chat', \n",
    "        'High Net Worth', 'Core Service', 'Core Service', 'High Net Worth', 'Core Service'\n",
    "    ],\n",
    "    'tenured': ['Yes'] * 15,\n",
    "    'talk_time': [\n",
    "        2353, 34945, 6693, 12467, 6444, \n",
    "        18897, 18897, 11115, 7036, 13667, \n",
    "        3786, 12718, 13221, 2695, 2200\n",
    "    ],\n",
    "    'wrap_time': [\n",
    "        749, 0, 1402, 4207, 84, \n",
    "        3399, 3399, 1736, 4361, 0, \n",
    "        27, 2010, 663, 27, 749\n",
    "    ],\n",
    "    'interaction_count': [\n",
    "        9, 51, 12, 26, 7, \n",
    "        33, 33, 14, 22, 18, \n",
    "        10, 15, 18, 9, 4\n",
    "    ],\n",
    "    'total_head_ct': [\n",
    "        2468, 2468, 2468, 2468, 2468, \n",
    "        2468, 2468, 2468, 2468, 2468, \n",
    "        2468, 2468, 2468, 2468, 2468\n",
    "    ],\n",
    "    'tenure_head_ct': [\n",
    "        1840, 1840, 1840, 1840, 1840, \n",
    "        1840, 1840, 1840, 1840, 1840, \n",
    "        1840, 1840, 1840, 1840, 1840\n",
    "    ],\n",
    "    'area_tenure_head_ct': [\n",
    "        241, 209, 241, 326, 241, \n",
    "        764, 764, 326, 326, 209, \n",
    "        241, 764, 764, 241, 764\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "result_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        area_name   tenured  head_count  sample_home  sample_office  aht_home  \\\n",
      "0        COMBINED  COMBINED        2468          144            137  694.1458   \n",
      "1        COMBINED       Yes        1840          144            137  694.1458   \n",
      "2            Chat       Yes         209           18             51  759.2778   \n",
      "3    Core Service       Yes         764           81             22  732.3457   \n",
      "4  High Net Worth       Yes         241            9             38  302.4444   \n",
      "5  Trader Service       Yes         326           36             26  673.5556   \n",
      "\n",
      "   aht_office  \n",
      "0    656.8613  \n",
      "1    656.8613  \n",
      "2    685.1961  \n",
      "3    765.1364  \n",
      "4    566.7895  \n",
      "5    641.3077  \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH CTE AS\n",
    "(\n",
    "SELECT \n",
    "  day_type\n",
    ", area_name\n",
    ", tenured\n",
    ", area_tenure_head_ct AS head_count\n",
    ", SUM(interaction_count) AS sample\n",
    ", ROUND(CAST(SUM(talk_time + wrap_time) AS FLOAT) / SUM(interaction_count), 4) AS aht\n",
    "FROM result_df\n",
    "WHERE interaction_count > 0\n",
    "GROUP BY 1, 2, 3, 4\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT \n",
    "  day_type\n",
    ", \"COMBINED\" AS area_name\n",
    ", tenured\n",
    ", tenure_head_ct AS head_count\n",
    ", SUM(interaction_count) AS sample\n",
    ", ROUND(CAST(SUM(talk_time + wrap_time) AS FLOAT) / SUM(interaction_count), 4) AS aht\n",
    "FROM result_df\n",
    "WHERE interaction_count > 0\n",
    "GROUP BY 1, 2, 3, 4\n",
    "\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT \n",
    "  day_type\n",
    ", \"COMBINED\" AS area_name\n",
    ", \"COMBINED\" AS tenured\n",
    ", total_head_ct AS head_count\n",
    ", SUM(interaction_count) AS sample\n",
    ", ROUND(CAST(SUM(talk_time + wrap_time) AS FLOAT) / SUM(interaction_count), 4) AS aht\n",
    "FROM result_df\n",
    "WHERE interaction_count > 0\n",
    "GROUP BY 1, 2, 3, 4\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  HOME.area_name\n",
    ", HOME.tenured\n",
    ", HOME.head_count\n",
    ", HOME.sample AS sample_home\n",
    ", OFFICE.sample AS sample_office\n",
    ", HOME.aht AS aht_home\n",
    ", OFFICE.aht AS aht_office\n",
    "FROM CTE HOME\n",
    "INNER JOIN CTE OFFICE\n",
    "    ON HOME.area_name = OFFICE.area_name\n",
    "    AND HOME.tenured = OFFICE.tenured\n",
    "    AND HOME.head_count = OFFICE.head_count\n",
    "    AND OFFICE.day_type = 'OFFICE'\n",
    "WHERE HOME.day_type = 'HOME'\n",
    "ORDER BY HOME.area_name, HOME.tenured\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "aht_df = sqldf(query)\n",
    "print(aht_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        area_name   tenured  head_count  sample_home  sample_office  aht_home  \\\n",
      "0        COMBINED  COMBINED        2468            6              8  694.1458   \n",
      "1        COMBINED       Yes        1840            6              8  694.1458   \n",
      "2            Chat       Yes         209            1              1  759.2778   \n",
      "3    Core Service       Yes         764            2              2  732.3457   \n",
      "4  High Net Worth       Yes         241            1              4  302.4444   \n",
      "\n",
      "   aht_office  sample_size_home  sample_size_office  \n",
      "0    656.8613                 6                   8  \n",
      "1    656.8613                 6                   8  \n",
      "2    685.1961                 1                   1  \n",
      "3    765.1364                 2                   2  \n",
      "4    566.7895                 1                   4  \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH CTE AS (\n",
    "\n",
    "SELECT \n",
    "  day_type\n",
    ", area_name\n",
    ", tenured\n",
    ", area_tenure_head_ct AS head_count\n",
    ", ROUND(CAST(SUM(talk_time + wrap_time) AS FLOAT) / SUM(interaction_count), 4) AS aht\n",
    ", COUNT(DISTINCT person_id || date) AS sample_size -- This calculates the sample size for each group\n",
    "FROM result_df\n",
    "WHERE interaction_count > 0\n",
    "GROUP BY 1, 2, 3, 4\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT \n",
    "  day_type\n",
    ", \"COMBINED\" AS area_name\n",
    ", tenured\n",
    ", tenure_head_ct AS head_count\n",
    ", ROUND(CAST(SUM(talk_time + wrap_time) AS FLOAT) / SUM(interaction_count), 4) AS aht\n",
    ", COUNT(DISTINCT person_id || date) AS sample_size -- This calculates the sample size for each group\n",
    "FROM result_df\n",
    "WHERE interaction_count > 0\n",
    "GROUP BY 1, 2, 3, 4\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT \n",
    "  day_type\n",
    ", \"COMBINED\" AS area_name\n",
    ", \"COMBINED\" AS tenured\n",
    ", total_head_ct AS head_count\n",
    ", ROUND(CAST(SUM(talk_time + wrap_time) AS FLOAT) / SUM(interaction_count), 4) AS aht\n",
    ", COUNT(DISTINCT person_id || date) AS sample_size -- This calculates the sample size for each group\n",
    "FROM result_df\n",
    "WHERE interaction_count > 0\n",
    "GROUP BY 1, 2, 3, 4\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  HOME.area_name\n",
    ", HOME.tenured\n",
    ", HOME.head_count\n",
    ", HOME.sample_size AS sample_home\n",
    ", OFFICE.sample_size AS sample_office\n",
    ", HOME.aht AS aht_home\n",
    ", OFFICE.aht AS aht_office\n",
    ", HOME.sample_size AS sample_size_home  -- Sample size for 'HOME' days\n",
    ", OFFICE.sample_size AS sample_size_office -- Sample size for 'OFFICE' days\n",
    "FROM CTE HOME\n",
    "INNER JOIN CTE OFFICE\n",
    "    ON HOME.area_name = OFFICE.area_name\n",
    "    AND HOME.tenured = OFFICE.tenured\n",
    "    AND HOME.head_count = OFFICE.head_count\n",
    "    AND OFFICE.day_type = 'OFFICE'\n",
    "WHERE HOME.day_type = 'HOME'\n",
    "ORDER BY HOME.area_name, HOME.tenured\n",
    "\"\"\"\n",
    "\n",
    "aht_df = sqldf(query)\n",
    "print(aht_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        area_name   tenured  head_count  sample_home  sample_office  aht_home  \\\n",
      "0        COMBINED  COMBINED        2468            6              8  694.1458   \n",
      "1        COMBINED       Yes        1840            6              8  694.1458   \n",
      "2            Chat       Yes         209            1              1  759.2778   \n",
      "3    Core Service       Yes         764            2              2  732.3457   \n",
      "4  High Net Worth       Yes         241            1              4  302.4444   \n",
      "5  Trader Service       Yes         326            2              1  673.5556   \n",
      "\n",
      "   aht_office  sample_size_home  sample_size_office    std_home  std_office  \\\n",
      "0    656.8613                 6                   8  283.383836  232.235540   \n",
      "1    656.8613                 6                   8  283.383836  232.235540   \n",
      "2    685.1961                 1                   1  759.277800  685.196100   \n",
      "3    765.1364                 2                   2  517.846611  541.033137   \n",
      "4    566.7895                 1                   4  302.444400  283.394750   \n",
      "5    641.3077                 2                   1  476.275732  641.307700   \n",
      "\n",
      "    t_value        df  p_value   cohen_d effect_size  \n",
      "0  0.262814  9.571034    0.798  0.146330       Small  \n",
      "1  0.262814  9.571034    0.798  0.146330       Small  \n",
      "2  0.072435  0.000000      NaN       NaN       Large  \n",
      "3 -0.061920  1.996175    0.956 -0.061920       Small  \n",
      "4 -0.791471  0.000000      NaN -0.932781       Large  \n",
      "5  0.044519  0.000000      NaN  0.067708       Small  \n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate the standard deviation for 'home' and 'office' AHTs\n",
    "aht_df['std_home'] = aht_df['aht_home'] / np.sqrt(aht_df['sample_home'])\n",
    "aht_df['std_office'] = aht_df['aht_office'] / np.sqrt(aht_df['sample_office'])\n",
    "\n",
    "# Calculate the t-value\n",
    "aht_df['t_value'] = (aht_df['aht_home'] - aht_df['aht_office']) / np.sqrt(\n",
    "    (aht_df['std_home']**2 / aht_df['sample_home']) + \n",
    "    (aht_df['std_office']**2 / aht_df['sample_office'])\n",
    ")\n",
    "\n",
    "# Calculate the degrees of freedom using the Welch-Satterthwaite equation\n",
    "aht_df['df'] = ((aht_df['std_home']**2 / aht_df['sample_home']) + \n",
    "                (aht_df['std_office']**2 / aht_df['sample_office']))**2 / (\n",
    "                ((aht_df['std_home']**2 / aht_df['sample_home'])**2 / (aht_df['sample_home'] - 1)) + \n",
    "                ((aht_df['std_office']**2 / aht_df['sample_office'])**2 / (aht_df['sample_office'] - 1))\n",
    "              )\n",
    "\n",
    "# Calculate the p-value using the t-distribution survival function\n",
    "aht_df['p_value'] = stats.t.sf(np.abs(aht_df['t_value']), aht_df['df']) * 2  # Two-tailed test\n",
    "\n",
    "# Round the p-value to 3 decimal places\n",
    "aht_df['p_value'] = aht_df['p_value'].round(3)\n",
    "\n",
    "\n",
    "# Calculate pooled standard deviation for Cohen's d\n",
    "pooled_std = np.sqrt(\n",
    "    ((aht_df['sample_home'] - 1) * aht_df['std_home']**2 + \n",
    "     (aht_df['sample_office'] - 1) * aht_df['std_office']**2) / \n",
    "    (aht_df['sample_home'] + aht_df['sample_office'] - 2)\n",
    ")\n",
    "\n",
    "# Calculate Cohen's d\n",
    "aht_df['cohen_d'] = (aht_df['aht_home'] - aht_df['aht_office']) / pooled_std\n",
    "\n",
    "# Function to interpret Cohen's d\n",
    "def interpret_cohen_d(d):\n",
    "    if abs(d) < 0.2:\n",
    "        return \"Small\"\n",
    "    elif abs(d) < 0.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\"\n",
    "\n",
    "# Apply the interpretation function to Cohen's d values\n",
    "aht_df['effect_size'] = aht_df['cohen_d'].apply(interpret_cohen_d)\n",
    "\n",
    "\n",
    "# Display the DataFrame with the new calculations\n",
    "print(aht_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   day_type       area_name   tenured  sample       aht  home_variance  \\\n",
      "0      HOME        COMBINED  COMBINED    2468  694.1458            0.0   \n",
      "1      HOME        COMBINED       Yes    1840  694.1458            0.0   \n",
      "2      HOME            Chat       Yes     209  759.2778            0.0   \n",
      "3      HOME    Core Service       Yes     764  732.3457            0.0   \n",
      "4      HOME  High Net Worth       Yes     241  302.4444            0.0   \n",
      "5      HOME  Trader Service       Yes     326  673.5556            0.0   \n",
      "6    OFFICE        COMBINED  COMBINED    2468  656.8613            0.0   \n",
      "7    OFFICE        COMBINED       Yes    1840  656.8613            0.0   \n",
      "8    OFFICE            Chat       Yes     209  685.1961            0.0   \n",
      "9    OFFICE    Core Service       Yes     764  765.1364            0.0   \n",
      "10   OFFICE  High Net Worth       Yes     241  566.7895            0.0   \n",
      "11   OFFICE  Trader Service       Yes     326  641.3077            0.0   \n",
      "\n",
      "    office_variance  \n",
      "0               0.0  \n",
      "1               0.0  \n",
      "2               0.0  \n",
      "3               0.0  \n",
      "4               0.0  \n",
      "5               0.0  \n",
      "6               0.0  \n",
      "7               0.0  \n",
      "8               0.0  \n",
      "9               0.0  \n",
      "10              0.0  \n",
      "11              0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming aht_df has already been created from your previous steps\n",
    "# Grouping by 'area_name', 'tenured', and 'day_type' to calculate variances\n",
    "\n",
    "# Create a function to calculate variance\n",
    "def calculate_variance(group):\n",
    "    sample_mean = group['aht'].mean()\n",
    "    sample_size = group['sample'].mean()  # We take mean as sample size for each subgroup\n",
    "    variance = np.sum((group['aht'] - sample_mean)**2) / (sample_size - 1)\n",
    "    return variance\n",
    "\n",
    "# Separate the data into 'HOME' and 'OFFICE' groups\n",
    "home_group = aht_df[aht_df['day_type'] == 'HOME']\n",
    "office_group = aht_df[aht_df['day_type'] == 'OFFICE']\n",
    "\n",
    "# Calculate variance for each group\n",
    "home_variances = home_group.groupby(['area_name', 'tenured']).apply(calculate_variance).reset_index(name='variance')\n",
    "office_variances = office_group.groupby(['area_name', 'tenured']).apply(calculate_variance).reset_index(name='variance')\n",
    "\n",
    "# Merge the variances back to the original aht_df\n",
    "home_variances = home_variances.rename(columns={'variance': 'home_variance'})\n",
    "office_variances = office_variances.rename(columns={'variance': 'office_variance'})\n",
    "\n",
    "# Now merging variances back with the main DataFrame\n",
    "aht_variance_df = pd.merge(aht_df, home_variances, on=['area_name', 'tenured'], how='left')\n",
    "aht_variance_df = pd.merge(aht_variance_df, office_variances, on=['area_name', 'tenured'], how='left')\n",
    "\n",
    "# Display the DataFrame with calculated variances\n",
    "print(aht_variance_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        area_name   tenured  head_count  sample_home  sample_office  aht_home  \\\n",
      "0        COMBINED  COMBINED        2468          144            137  694.1458   \n",
      "1        COMBINED       Yes        1840          144            137  694.1458   \n",
      "2            Chat       Yes         209           18             51  759.2778   \n",
      "3    Core Service       Yes         764           81             22  732.3457   \n",
      "4  High Net Worth       Yes         241            9             38  302.4444   \n",
      "\n",
      "   aht_office  \n",
      "0    656.8613  \n",
      "1    656.8613  \n",
      "2    685.1961  \n",
      "3    765.1364  \n",
      "4    566.7895  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "\n",
    "# Data for the DataFrame (as you provided earlier)\n",
    "data = {\n",
    "    'day_type': [\n",
    "        'OFFICE', 'OFFICE', 'OFFICE', 'OFFICE', 'OFFICE',\n",
    "        'HOME', 'HOME', 'HOME', 'HOME', 'HOME',\n",
    "        'OFFICE', 'HOME', 'OFFICE', 'HOME', 'OFFICE'\n",
    "    ],\n",
    "    'date': [\n",
    "        '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03', \n",
    "        '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03', \n",
    "        '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03', '2024-01-03'\n",
    "    ],\n",
    "    'person_id': [\n",
    "        'A003083', 'A003091', 'A003434', 'A003687', 'A003771', \n",
    "        'A004173', 'A004173', 'A004432', 'A005648', 'A007225', \n",
    "        'A008689', 'A009607', 'A009687', 'A015864', 'A017412'\n",
    "    ],\n",
    "    'area_name': [\n",
    "        'High Net Worth', 'Chat', 'High Net Worth', 'Trader Service', 'High Net Worth', \n",
    "        'Core Service', 'Core Service', 'Trader Service', 'Trader Service', 'Chat', \n",
    "        'High Net Worth', 'Core Service', 'Core Service', 'High Net Worth', 'Core Service'\n",
    "    ],\n",
    "    'tenured': ['Yes'] * 15,\n",
    "    'talk_time': [\n",
    "        2353, 34945, 6693, 12467, 6444, \n",
    "        18897, 18897, 11115, 7036, 13667, \n",
    "        3786, 12718, 13221, 2695, 2200\n",
    "    ],\n",
    "    'wrap_time': [\n",
    "        749, 0, 1402, 4207, 84, \n",
    "        3399, 3399, 1736, 4361, 0, \n",
    "        27, 2010, 663, 27, 749\n",
    "    ],\n",
    "    'interaction_count': [\n",
    "        9, 51, 12, 26, 7, \n",
    "        33, 33, 14, 22, 18, \n",
    "        10, 15, 18, 9, 4\n",
    "    ],\n",
    "    'total_head_ct': [\n",
    "        2468, 2468, 2468, 2468, 2468, \n",
    "        2468, 2468, 2468, 2468, 2468, \n",
    "        2468, 2468, 2468, 2468, 2468\n",
    "    ],\n",
    "    'tenure_head_ct': [\n",
    "        1840, 1840, 1840, 1840, 1840, \n",
    "        1840, 1840, 1840, 1840, 1840, \n",
    "        1840, 1840, 1840, 1840, 1840\n",
    "    ],\n",
    "    'area_tenure_head_ct': [\n",
    "        241, 209, 241, 326, 241, \n",
    "        764, 764, 326, 326, 209, \n",
    "        241, 764, 764, 241, 764\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "result_df = pd.DataFrame(data)\n",
    "\n",
    "# SQL query\n",
    "query = \"\"\"\n",
    "WITH CTE AS\n",
    "(\n",
    "SELECT \n",
    "  day_type\n",
    ", area_name\n",
    ", tenured\n",
    ", area_tenure_head_ct AS head_count\n",
    ", SUM(interaction_count) AS sample\n",
    ", ROUND(CAST(SUM(talk_time + wrap_time) AS FLOAT) / SUM(interaction_count), 4) AS aht\n",
    "FROM result_df\n",
    "WHERE interaction_count > 0\n",
    "GROUP BY 1, 2, 3, 4\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT \n",
    "  day_type\n",
    ", \"COMBINED\" AS area_name\n",
    ", tenured\n",
    ", tenure_head_ct AS head_count\n",
    ", SUM(interaction_count) AS sample\n",
    ", ROUND(CAST(SUM(talk_time + wrap_time) AS FLOAT) / SUM(interaction_count), 4) AS aht\n",
    "FROM result_df\n",
    "WHERE interaction_count > 0\n",
    "GROUP BY 1, 2, 3, 4\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT \n",
    "  day_type\n",
    ", \"COMBINED\" AS area_name\n",
    ", \"COMBINED\" AS tenured\n",
    ", total_head_ct AS head_count\n",
    ", SUM(interaction_count) AS sample\n",
    ", ROUND(CAST(SUM(talk_time + wrap_time) AS FLOAT) / SUM(interaction_count), 4) AS aht\n",
    "FROM result_df\n",
    "WHERE interaction_count > 0\n",
    "GROUP BY 1, 2, 3, 4\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  HOME.area_name\n",
    ", HOME.tenured\n",
    ", HOME.head_count\n",
    ", HOME.sample AS sample_home\n",
    ", OFFICE.sample AS sample_office\n",
    ", HOME.aht AS aht_home\n",
    ", OFFICE.aht AS aht_office\n",
    "FROM CTE HOME\n",
    "INNER JOIN CTE OFFICE\n",
    "    ON HOME.area_name = OFFICE.area_name\n",
    "    AND HOME.tenured = OFFICE.tenured\n",
    "    AND HOME.head_count = OFFICE.head_count\n",
    "    AND OFFICE.day_type = 'OFFICE'\n",
    "WHERE HOME.day_type = 'HOME'\n",
    "ORDER BY HOME.area_name, HOME.tenured\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "aht_df = sqldf(query)\n",
    "\n",
    "# Display the first 5 rows of the result\n",
    "print(aht_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        area_name   tenured  head_count  sample_home  sample_office  aht_home  \\\n",
      "0        COMBINED  COMBINED        2468          144            137  694.1458   \n",
      "1        COMBINED       Yes        1840          144            137  694.1458   \n",
      "2            Chat       Yes         209           18             51  759.2778   \n",
      "3    Core Service       Yes         764           81             22  732.3457   \n",
      "4  High Net Worth       Yes         241            9             38  302.4444   \n",
      "5  Trader Service       Yes         326           36             26  673.5556   \n",
      "\n",
      "   aht_office   t_value  p_value  cohen_d effect_size  \n",
      "0    656.8613  7.429510  0.00000  0.83969       Large  \n",
      "1    656.8613  7.429510  0.00000  0.83969       Large  \n",
      "2    685.1961  5.679357  0.00001  0.83969       Large  \n",
      "3    765.1364 -2.722768  0.01194  0.83969       Large  \n",
      "4    566.7895 -4.749968  0.00051  0.83969       Large  \n",
      "5    641.3077  3.311230  0.00162  0.83969       Large  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Assuming you have your DataFrame `aht_df`\n",
    "\n",
    "# Initialize lists to store results\n",
    "t_values = []\n",
    "p_values = []\n",
    "effect_sizes = []\n",
    "\n",
    "# Loop through each row in the DataFrame and perform the t-test\n",
    "for index, row in aht_df.iterrows():\n",
    "    # Generate random samples based on the given AHT and sample size\n",
    "    np.random.seed(0)  # For reproducibility\n",
    "    sample_home = np.random.normal(row['aht_home'], row['aht_home']/np.sqrt(row['sample_home']), int(row['sample_home']))\n",
    "    sample_office = np.random.normal(row['aht_office'], row['aht_office']/np.sqrt(row['sample_office']), int(row['sample_office']))\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_stat, p_val = ttest_ind(sample_home, sample_office, equal_var=False)\n",
    "    \n",
    "    # Calculate Cohen's d\n",
    "    pooled_std = np.sqrt(((len(sample_home) - 1) * np.std(sample_home, ddof=1) ** 2 + \n",
    "                          (len(sample_office) - 1) * np.std(sample_office, ddof=1) ** 2) / \n",
    "                         (len(sample_home) + len(sample_office) - 2))\n",
    "    cohen_d = (np.mean(sample_home) - np.mean(sample_office)) / pooled_std\n",
    "    \n",
    "    # Interpret Cohen's d\n",
    "    if abs(cohen_d) < 0.2:\n",
    "        effect_size = \"Small\"\n",
    "    elif abs(cohen_d) < 0.5:\n",
    "        effect_size = \"Medium\"\n",
    "    else:\n",
    "        effect_size = \"Large\"\n",
    "    \n",
    "    # Store results\n",
    "    t_values.append(t_stat)\n",
    "    p_values.append(p_val)\n",
    "    effect_sizes.append(effect_size)\n",
    "\n",
    "# Add results to the DataFrame\n",
    "aht_df['t_value'] = t_values\n",
    "aht_df['p_value'] = np.round(p_values, 5)\n",
    "aht_df['cohen_d'] = np.round(cohen_d, 5)\n",
    "aht_df['effect_size'] = effect_sizes\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(aht_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewenv",
   "language": "python",
   "name": "mynewenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
