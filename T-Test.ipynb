{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2f0698bc1d18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;31m# Perform the T-Tests and get the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m \u001b[0mt_test_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperform_t_tests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_test_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "\n",
    "# Define the function to perform T-Tests and calculate Cohen's d\n",
    "def perform_t_tests(df, metrics, alpha=0.05):\n",
    "    # Get unique values for 'area' and 'tenured'\n",
    "    areas = df['area'].unique()\n",
    "    tenured_statuses = df['tenured'].unique()\n",
    "    \n",
    "    # Function to interpret Cohen's d\n",
    "    def interpret_cohen_d(d):\n",
    "        if abs(d) < 0.2:\n",
    "            return \"Small\"\n",
    "        elif abs(d) < 0.5:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Large\"\n",
    "\n",
    "    # Initialize a list to store results\n",
    "    results = []\n",
    "\n",
    "    for tenured in tenured_statuses:\n",
    "        for area in areas:\n",
    "            for metric in metrics:\n",
    "                # Subset data based on 'area' and 'tenured'\n",
    "                subset = df[(df['tenured'] == tenured) & (df['area'] == area)]\n",
    "                # Separate data into 'HOME' and 'OFFICE' groups\n",
    "                wfh_data = subset[subset['day_type'] == 'HOME'][metric]\n",
    "                office_data = subset[subset['day_type'] == 'OFFICE'][metric]\n",
    "                \n",
    "                if len(wfh_data) > 0 and len(office_data) > 0:\n",
    "                    # Perform T-Test\n",
    "                    t_stat, p_value = ttest_ind(wfh_data, office_data, equal_var=False, nan_policy='omit')\n",
    "                    \n",
    "                    # Calculate means and sample sizes\n",
    "                    wfh_mean = wfh_data.mean()\n",
    "                    office_mean = office_data.mean()\n",
    "                    wfh_std = wfh_data.std()\n",
    "                    office_std = office_data.std()\n",
    "                    n_wfh = len(wfh_data)\n",
    "                    n_office = len(office_data)\n",
    "                    \n",
    "                    # Calculate pooled standard deviation\n",
    "                    pooled_std = np.sqrt(((n_wfh - 1) * wfh_std**2 + (n_office - 1) * office_std**2) / (n_wfh + n_office - 2))\n",
    "                    \n",
    "                    # Calculate Cohen's d\n",
    "                    cohen_d = (wfh_mean - office_mean) / pooled_std\n",
    "                    cohen_d_interpretation = interpret_cohen_d(cohen_d)\n",
    "                    \n",
    "                    # Append results to the list\n",
    "                    results.append({\n",
    "                        'Tenured': tenured,\n",
    "                        'Area': area,\n",
    "                        'Metric': metric,\n",
    "                        'WFH Mean': wfh_mean,\n",
    "                        'Office Mean': office_mean,\n",
    "                        'T-Statistic': t_stat,\n",
    "                        'P-Value': round(p_value, 5),\n",
    "                        'Cohen D': round(cohen_d, 5),\n",
    "                        'Cohen D Interpretation': cohen_d_interpretation\n",
    "                    })\n",
    "        \n",
    "        # Add combined area for the current tenured status\n",
    "        for metric in metrics:\n",
    "            combined_subset = df[df['tenured'] == tenured]\n",
    "            wfh_data_combined = combined_subset[combined_subset['day_type'] == 'HOME'][metric]\n",
    "            office_data_combined = combined_subset[combined_subset['day_type'] == 'OFFICE'][metric]\n",
    "            \n",
    "            if len(wfh_data_combined) > 0 and len(office_data_combined) > 0:\n",
    "                # Perform T-Test\n",
    "                t_stat_combined, p_value_combined = ttest_ind(wfh_data_combined, office_data_combined, equal_var=False, nan_policy='omit')\n",
    "                \n",
    "                # Calculate means and sample sizes\n",
    "                wfh_mean_combined = wfh_data_combined.mean()\n",
    "                office_mean_combined = office_data_combined.mean()\n",
    "                wfh_std_combined = wfh_data_combined.std()\n",
    "                office_std_combined = office_data_combined.std()\n",
    "                n_wfh_combined = len(wfh_data_combined)\n",
    "                n_office_combined = len(office_data_combined)\n",
    "                \n",
    "                # Calculate pooled standard deviation\n",
    "                pooled_std_combined = np.sqrt(((n_wfh_combined - 1) * wfh_std_combined**2 + (n_office_combined - 1) * office_std_combined**2) / (n_wfh_combined + n_office_combined - 2))\n",
    "                \n",
    "                # Calculate Cohen's d\n",
    "                cohen_d_combined = (wfh_mean_combined - office_mean_combined) / pooled_std_combined\n",
    "                cohen_d_combined_interpretation = interpret_cohen_d(cohen_d_combined)\n",
    "                \n",
    "                # Append combined area results to the list\n",
    "                results.append({\n",
    "                    'Tenured': tenured,\n",
    "                    'Area': 'All Areas Combined',\n",
    "                    'Metric': metric,\n",
    "                    'WFH Mean': wfh_mean_combined,\n",
    "                    'Office Mean': office_mean_combined,\n",
    "                    'T-Statistic': t_stat_combined,\n",
    "                    'P-Value': round(p_value_combined, 5),\n",
    "                    'Cohen D': round(cohen_d_combined, 5),\n",
    "                    'Cohen D Interpretation': cohen_d_combined_interpretation\n",
    "                })\n",
    "    \n",
    "    # Convert results list to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Assuming 'result_df' is your DataFrame loaded with your data\n",
    "metrics = ['avg_interaction_count', 'aht', 'avg_productivity']\n",
    "\n",
    "# Set the alpha level (e.g., 0.05 for a 95% confidence level)\n",
    "alpha = 0.05\n",
    "\n",
    "# Perform the T-Tests and get the results\n",
    "t_test_results = perform_t_tests(result_df, metrics)\n",
    "\n",
    "print(t_test_results.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tenured                Area Name  Sample     Metric  WFH Proportion  \\\n",
      "0      No                     Chat       4  Adherence           0.755   \n",
      "1      No                     Chat       4   Consults           0.153   \n",
      "2      No                     Chat       4       OSAT           0.552   \n",
      "3      No                     Chat       4  Transfers           0.129   \n",
      "4      No  Client Banking Services      46  Adherence           0.912   \n",
      "\n",
      "   Office Proportion  Z-Statistic       P-Value Cohen H Interpretation  \\\n",
      "0           0.374540     1.085247  2.778122e-01                  Large   \n",
      "1           0.950714    -2.268511  2.329811e-02                  Large   \n",
      "2           0.731994    -0.530962  5.954454e-01                 Medium   \n",
      "3           0.598658    -1.380580  1.674082e-01                  Large   \n",
      "4           0.156019     7.267951  3.649820e-13                  Large   \n",
      "\n",
      "   Significant  \n",
      "0        False  \n",
      "1         True  \n",
      "2        False  \n",
      "3        False  \n",
      "4         True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from math import asin, sqrt\n",
    "\n",
    "# Transcribe the data into a list of dictionaries\n",
    "data = [\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Chat\", \"Sample\": 4, \"Metric\": \"Adherence\", \"WFH Proportion\": 0.755},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Chat\", \"Sample\": 4, \"Metric\": \"Consults\", \"WFH Proportion\": 0.153},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Chat\", \"Sample\": 4, \"Metric\": \"OSAT\", \"WFH Proportion\": 0.552},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Chat\", \"Sample\": 4, \"Metric\": \"Transfers\", \"WFH Proportion\": 0.129},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Client Banking Services\", \"Sample\": 46, \"Metric\": \"Adherence\", \"WFH Proportion\": 0.912},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Client Banking Services\", \"Sample\": 46, \"Metric\": \"Consults\", \"WFH Proportion\": 0.206},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Client Banking Services\", \"Sample\": 46, \"Metric\": \"OSAT\", \"WFH Proportion\": 0.472},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Client Banking Services\", \"Sample\": 46, \"Metric\": \"Transfers\", \"WFH Proportion\": 0.148},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Core Service\", \"Sample\": 594, \"Metric\": \"Adherence\", \"WFH Proportion\": 0.836},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Core Service\", \"Sample\": 594, \"Metric\": \"Consults\", \"WFH Proportion\": 0.178},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Core Service\", \"Sample\": 594, \"Metric\": \"OSAT\", \"WFH Proportion\": 0.468},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Core Service\", \"Sample\": 594, \"Metric\": \"Transfers\", \"WFH Proportion\": 0.188},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Tier 2 Support\", \"Sample\": 5, \"Metric\": \"Adherence\", \"WFH Proportion\": 0.888},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Tier 2 Support\", \"Sample\": 5, \"Metric\": \"Consults\", \"WFH Proportion\": 0.104},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Tier 2 Support\", \"Sample\": 5, \"Metric\": \"OSAT\", \"WFH Proportion\": 1.000},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Tier 2 Support\", \"Sample\": 5, \"Metric\": \"Transfers\", \"WFH Proportion\": 0.241},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Trader Service\", \"Sample\": 6, \"Metric\": \"Adherence\", \"WFH Proportion\": 0.822},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Trader Service\", \"Sample\": 6, \"Metric\": \"Consults\", \"WFH Proportion\": 0.266},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Trader Service\", \"Sample\": 6, \"Metric\": \"OSAT\", \"WFH Proportion\": 0.571},\n",
    "    {\"Tenured\": \"No\", \"Area Name\": \"Trader Service\", \"Sample\": 6, \"Metric\": \"Transfers\", \"WFH Proportion\": 0.273},\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Adding some hypothetical Office Proportion data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "df[\"Office Proportion\"] = np.random.rand(len(df))\n",
    "\n",
    "# Set the alpha level (e.g., 0.05 for a 95% confidence level)\n",
    "alpha = 0.05\n",
    "\n",
    "# Function to interpret Cohen's h\n",
    "def interpret_cohen_h(h):\n",
    "    if abs(h) < 0.2:\n",
    "        return \"Small\"\n",
    "    elif abs(h) < 0.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\"\n",
    "\n",
    "# Perform Z-tests for each row\n",
    "results = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    count = np.array([row[\"WFH Proportion\"] * row[\"Sample\"], row[\"Office Proportion\"] * row[\"Sample\"]])\n",
    "    nobs = np.array([row[\"Sample\"], row[\"Sample\"]])\n",
    "    \n",
    "    z_score, p_value = proportions_ztest(count, nobs)\n",
    "    \n",
    "    # Calculate Cohen's h\n",
    "    cohen_h = 2 * (asin(sqrt(row[\"WFH Proportion\"])) - asin(sqrt(row[\"Office Proportion\"])))\n",
    "    \n",
    "    results.append({\n",
    "        \"Tenured\": row[\"Tenured\"],\n",
    "        \"Area Name\": row[\"Area Name\"],\n",
    "        \"Sample\": row[\"Sample\"],\n",
    "        \"Metric\": row[\"Metric\"],\n",
    "        \"WFH Proportion\": row[\"WFH Proportion\"],\n",
    "        \"Office Proportion\": row[\"Office Proportion\"],\n",
    "        \"Z-Statistic\": z_score,\n",
    "        \"P-Value\": p_value,\n",
    "        \"Cohen H\": cohen_h,\n",
    "        \"Cohen H Interpretation\": interpret_cohen_h(cohen_h),\n",
    "        \"Significant\": p_value < alpha  # Check if the p-value is less than the alpha level\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Add combined area for each tenured status\n",
    "combined_results = []\n",
    "\n",
    "for tenured in df['Tenured'].unique():\n",
    "    for metric in df['Metric'].unique():\n",
    "        combined_subset = df[(df['Tenured'] == tenured) & (df['Metric'] == metric)]\n",
    "        combined_sample = combined_subset['Sample'].sum()\n",
    "        combined_wfh_proportion = (combined_subset['WFH Proportion'] * combined_subset['Sample']).sum() / combined_sample\n",
    "        combined_office_proportion = (combined_subset['Office Proportion'] * combined_subset['Sample']).sum() / combined_sample\n",
    "        \n",
    "        count_combined = np.array([combined_wfh_proportion * combined_sample, combined_office_proportion * combined_sample])\n",
    "        nobs_combined = np.array([combined_sample, combined_sample])\n",
    "        \n",
    "        z_score_combined, p_value_combined = proportions_ztest(count_combined, nobs_combined)\n",
    "        \n",
    "        # Calculate Cohen's h for combined data\n",
    "        cohen_h_combined = 2 * (asin(sqrt(combined_wfh_proportion)) - asin(sqrt(combined_office_proportion)))\n",
    "        \n",
    "        combined_results.append({\n",
    "            \"Tenured\": tenured,\n",
    "            \"Area Name\": \"COMBINED\",\n",
    "            \"Sample\": combined_sample,\n",
    "            \"Metric\": metric,\n",
    "            \"WFH Proportion\": combined_wfh_proportion,\n",
    "            \"Office Proportion\": combined_office_proportion,\n",
    "            \"Z-Statistic\": z_score_combined,\n",
    "            \"P-Value\": p_value_combined,\n",
    "            \"Cohen H\": cohen_h_combined,\n",
    "            \"Cohen H Interpretation\": interpret_cohen_h(cohen_h_combined),\n",
    "            \"Significant\": p_value_combined < alpha  # Check if the p-value is less than the alpha level\n",
    "        })\n",
    "\n",
    "# Convert combined results to a DataFrame and append to the original results\n",
    "df_combined_results = pd.DataFrame(combined_results)\n",
    "df_final_results = pd.concat([df_results, df_combined_results], ignore_index=True)\n",
    "\n",
    "# Reorder the columns\n",
    "df_final_results = df_final_results[[\"Tenured\", \"Area Name\", \"Sample\", \"Metric\", \"WFH Proportion\", \"Office Proportion\", \"Z-Statistic\", \"P-Value\", \"Cohen H Interpretation\", \"Significant\"]]\n",
    "\n",
    "print(df_final_results.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-af4e1299c9be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m }\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m \u001b[0marea_tenure_z_test_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marea_tenure_z_tests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[0mcombined_z_test_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_z_tests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from math import asin, sqrt\n",
    "\n",
    "# Define WFH and Office days\n",
    "wfh_days = ['Tuesday', 'Friday']\n",
    "office_days = ['Monday', 'Wednesday', 'Thursday']\n",
    "\n",
    "# Function to interpret Cohen's h\n",
    "def interpret_cohen_h(h):\n",
    "    if abs(h) < 0.2:\n",
    "        return \"Small\"\n",
    "    elif abs(h) < 0.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\"\n",
    "\n",
    "# Function to get the overall difference in proportions by area and tenure\n",
    "def area_tenure_z_tests(df, metrics_dict):\n",
    "    area_names = df['area_name'].unique()\n",
    "    tenured_statuses = df['tenured'].unique()\n",
    "    results = []\n",
    "\n",
    "    for tenured in tenured_statuses:\n",
    "        for area in area_names:\n",
    "            for metric, (num_col, den_col) in metrics_dict.items():\n",
    "                subset = df[(df['tenured'] == tenured) & (df['area_name'] == area)]\n",
    "                wfh_subset = subset[subset['day'].isin(wfh_days)]\n",
    "                office_subset = subset[subset['day'].isin(office_days)]\n",
    "                wfh_numerator = wfh_subset[num_col].sum()\n",
    "                wfh_denominator = wfh_subset[den_col].sum()\n",
    "                office_numerator = office_subset[num_col].sum()\n",
    "                office_denominator = office_subset[den_col].sum()\n",
    "\n",
    "                if wfh_denominator == 0 or office_denominator == 0:\n",
    "                    continue\n",
    "\n",
    "                count = np.array([wfh_numerator, office_numerator])\n",
    "                observations = np.array([wfh_denominator, office_denominator])\n",
    "                z_stat, p_value = proportions_ztest(count, observations)\n",
    "                wfh_proportion = wfh_numerator / wfh_denominator if wfh_denominator else None\n",
    "                office_proportion = office_numerator / office_denominator if office_denominator else None\n",
    "\n",
    "                # Calculate Cohen's h\n",
    "                cohen_h = 2 * (asin(sqrt(wfh_proportion)) - asin(sqrt(office_proportion)))\n",
    "                cohen_h_interpretation = interpret_cohen_h(cohen_h)\n",
    "\n",
    "                results.append({\n",
    "                    'Tenured': tenured,\n",
    "                    'Area Name': area,\n",
    "                    'Metric': metric,\n",
    "                    'WFH Proportion': wfh_proportion,\n",
    "                    'Office Proportion': office_proportion,\n",
    "                    'Z-Statistic': z_stat,\n",
    "                    'P-Value': p_value.round(5),\n",
    "                    'Cohen H': round(cohen_h, 5),\n",
    "                    'Cohen H Interpretation': cohen_h_interpretation\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Function to get the overall difference in proportions by tenure (area agnostic)\n",
    "def combined_z_tests(df, metrics_dict):\n",
    "    tenured_statuses = df['tenured'].unique()\n",
    "    results = []\n",
    "\n",
    "    for tenured in tenured_statuses:\n",
    "        for metric, (num_col, den_col) in metrics_dict.items():\n",
    "            subset = df[df['tenured'] == tenured]\n",
    "            wfh_subset = subset[subset['day'].isin(wfh_days)]\n",
    "            office_subset = subset[subset['day'].isin(office_days)]\n",
    "            wfh_numerator = wfh_subset[num_col].sum()\n",
    "            wfh_denominator = wfh_subset[den_col].sum()\n",
    "            office_numerator = office_subset[num_col].sum()\n",
    "            office_denominator = office_subset[den_col].sum()\n",
    "\n",
    "            if wfh_denominator == 0 or office_denominator == 0:\n",
    "                continue\n",
    "\n",
    "            count = np.array([wfh_numerator, office_numerator])\n",
    "            observations = np.array([wfh_denominator, office_denominator])\n",
    "            z_stat, p_value = proportions_ztest(count, observations)\n",
    "            wfh_proportion = wfh_numerator / wfh_denominator if wfh_denominator else None\n",
    "            office_proportion = office_numerator / office_denominator if office_denominator else None\n",
    "\n",
    "            # Calculate Cohen's h\n",
    "            cohen_h = 2 * (asin(sqrt(wfh_proportion)) - asin(sqrt(office_proportion)))\n",
    "            cohen_h_interpretation = interpret_cohen_h(cohen_h)\n",
    "\n",
    "            results.append({\n",
    "                'Tenured': tenured,\n",
    "                'Area Name': \"COMBINED\",\n",
    "                'Metric': metric,\n",
    "                'WFH Proportion': wfh_proportion,\n",
    "                'Office Proportion': office_proportion,\n",
    "                'Z-Statistic': z_stat,\n",
    "                'P-Value': p_value.round(5),\n",
    "                'Cohen H': round(cohen_h, 5),\n",
    "                'Cohen H Interpretation': cohen_h_interpretation\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test\n",
    "\n",
    "metrics_dict = {\n",
    "    'Adherence': ('adh_num', 'adh_den'),\n",
    "    'OSAT': ('top_box', 'osat_count'),\n",
    "    'Transfers': ('transfers', 'interaction_count'),\n",
    "    'Consults': ('consults', 'interaction_count')\n",
    "}\n",
    "\n",
    "area_tenure_z_test_df = area_tenure_z_tests(result_df, metrics_dict)\n",
    "combined_z_test_df = combined_z_tests(result_df, metrics_dict)\n",
    "\n",
    "z_test = pd.concat([area_tenure_z_test_df, combined_z_test_df])\n",
    "\n",
    "print(z_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to perform T-Tests and calculate Cohen's d\n",
    "def perform_t_tests(df, metrics, alpha=0.05):\n",
    "\n",
    "    # Get unique values for 'area' and 'tenured'\n",
    "    areas = df['area_name'].unique()\n",
    "    tenured_statuses = df['tenured'].unique()\n",
    "\n",
    "    # Function to interpret Cohen's d\n",
    "    def interpret_cohen_d(d):\n",
    "        if abs(d) < 0.2:\n",
    "            return \"Small\"\n",
    "        elif abs(d) < 0.5:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Large\"\n",
    "\n",
    "    # Initialize a list to store results\n",
    "    results = []\n",
    "\n",
    "    for tenured in tenured_statuses:\n",
    "        for area in areas:\n",
    "            for metric in metrics:\n",
    "                # Subset data based on 'area' and 'tenured'\n",
    "                subset = df[(df['tenured'] == tenured) & (df['area_name'] == area)]\n",
    "\n",
    "                if metric == 'aht':\n",
    "                    # Calculate the new average of AHT using internal components\n",
    "                    subset = subset[subset['interaction_count'] > 0]\n",
    "                    subset['new_aht'] = (subset['talk_time'] + subset['wrap_time']).sum() / subset['interaction_count'].sum()\n",
    "                    wfh_data = subset[subset['day_type'] == 'HOME']['new_aht']\n",
    "                    office_data = subset[subset['day_type'] == 'OFFICE']['new_aht']\n",
    "                else:\n",
    "                    # Separate data into 'HOME' and 'OFFICE' groups\n",
    "                    wfh_data = subset[subset['day_type'] == 'HOME'][metric]\n",
    "                    office_data = subset[subset['day_type'] == 'OFFICE'][metric]\n",
    "\n",
    "                if len(wfh_data) > 0 and len(office_data) > 0:\n",
    "                    # Perform T-Test\n",
    "                    t_stat, p_value = ttest_ind(wfh_data, office_data, equal_var=False, nan_policy='omit')\n",
    "\n",
    "                    # Calculate means and sample sizes\n",
    "                    wfh_mean = wfh_data.mean()\n",
    "                    office_mean = office_data.mean()\n",
    "                    wfh_std = wfh_data.std()\n",
    "                    office_std = office_data.std()\n",
    "                    n_wfh = len(wfh_data)\n",
    "                    n_office = len(office_data)\n",
    "\n",
    "                    # Calculate pooled standard deviation\n",
    "                    pooled_std = np.sqrt(((n_wfh - 1) * wfh_std**2 + (n_office - 1) * office_std**2) / (n_wfh + n_office - 2))\n",
    "\n",
    "                    # Calculate Cohen's d\n",
    "                    cohen_d = (wfh_mean - office_mean) / pooled_std\n",
    "                    cohen_d_interpretation = interpret_cohen_d(cohen_d)\n",
    "\n",
    "                    # Append results to the list\n",
    "                    results.append({\n",
    "                        'Tenured': tenured,\n",
    "                        'Area': area,\n",
    "                        'Metric': metric,\n",
    "                        'WFH Mean': wfh_mean,\n",
    "                        'Office Mean': office_mean,\n",
    "                        'T-Statistic': t_stat,\n",
    "                        'P-Value': round(p_value, 5),\n",
    "                        # 'Cohen D': round(cohen_d, 5),\n",
    "                        'Effect Size': cohen_d_interpretation\n",
    "                    })\n",
    "\n",
    "        # Add combined area for the current tenured status\n",
    "        for metric in metrics:\n",
    "            combined_subset = df[df['tenured'] == tenured]\n",
    "            if metric == 'aht':\n",
    "                combined_subset = combined_subset[combined_subset['interaction_count'] > 0]\n",
    "                combined_subset['new_aht'] = (combined_subset['talk_time'] + combined_subset['wrap_time']).sum() / combined_subset['interaction_count'].sum()\n",
    "                wfh_data_combined = combined_subset[combined_subset['day_type'] == 'HOME']['new_aht']\n",
    "                office_data_combined = combined_subset[combined_subset['day_type'] == 'OFFICE']['new_aht']\n",
    "            else:\n",
    "                wfh_data_combined = combined_subset[combined_subset['day_type'] == 'HOME'][metric]\n",
    "                office_data_combined = combined_subset[combined_subset['day_type'] == 'OFFICE'][metric]\n",
    "\n",
    "            if len(wfh_data_combined) > 0 and len(office_data_combined) > 0:\n",
    "                # Perform T-Test\n",
    "                t_stat_combined, p_value_combined = ttest_ind(wfh_data_combined, office_data_combined, equal_var=False, nan_policy='omit')\n",
    "\n",
    "                # Calculate means and sample sizes\n",
    "                wfh_mean_combined = wfh_data_combined.mean()\n",
    "                office_mean_combined = office_data_combined.mean()\n",
    "                wfh_std_combined = wfh_data_combined.std()\n",
    "                office_std_combined = office_data_combined.std()\n",
    "                n_wfh_combined = len(wfh_data_combined)\n",
    "                n_office_combined = len(office_data_combined)\n",
    "\n",
    "                # Calculate pooled standard deviation\n",
    "                pooled_std_combined = np.sqrt(((n_wfh_combined - 1) * wfh_std_combined**2 + (n_office_combined - 1) * office_std_combined**2) / (n_wfh_combined + n_office_combined - 2))\n",
    "\n",
    "                # Calculate Cohen's d\n",
    "                cohen_d_combined = (wfh_mean_combined - office_mean_combined) / pooled_std_combined\n",
    "                cohen_d_combined_interpretation = interpret_cohen_d(cohen_d_combined)\n",
    "\n",
    "                # Append combined area results to the list\n",
    "                results.append({\n",
    "                    'Tenured': tenured,\n",
    "                    'Area': 'COMBINED',\n",
    "                    'Metric': metric,\n",
    "                    'WFH Mean': wfh_mean_combined,\n",
    "                    'Office Mean': office_mean_combined,\n",
    "                    'T-Statistic': t_stat_combined,\n",
    "                    'P-Value': round(p_value_combined, 5),\n",
    "                    # 'Cohen D': round(cohen_d_combined, 5),\n",
    "                    'Effect Size': cohen_d_combined_interpretation\n",
    "                })\n",
    "\n",
    "    # Convert results list to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Assuming 'result_df' is your DataFrame loaded with your data\n",
    "metrics = ['interaction_ct', 'aht', 'net productivity']\n",
    "# Set the alpha level (e.g., 0.05 for a 95% confidence level)\n",
    "alpha = 0.05\n",
    "\n",
    "# Perform the T-Tests and get the results\n",
    "t_test_results = perform_t_tests(result_df, metrics)\n",
    "\n",
    "print(t_test_results.head(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
